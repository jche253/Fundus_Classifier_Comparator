
This project allows people to compare images to impose an order on the set of images. For example, retinal scans of premature babies show various degrees of severity of retinopathy. By allowing experts to select in a pairwise fashion, which image shows worse signs of retinopathy, an order is imposed on the set of images.

The goal of the software system is to provide, at a publicly accessible (but potentially password protected) website, pairs of images that a user can compare. The software will collect the results of these comparisons in a database. The software will allow for some people to configure a set of images that should be compared and assign a set of comparisons to particular users. It should be possible to configure sophisticated algorithms to select the next pair of images for comparison based on the results either from this user, or from other users. The software is able to then provide a variety of metrics on the results, for example, a metric to determine if the comparisons from a particular user internally consistent.

The technologies that the system is currently using are:

  html5
  javascript
    jquery
    D3
    bootstrap
  ruby
  couchdb
  aws
  git
  github

The system can be thought of as a client side website, called the front-end, and a server side database called the back-end. The front-end has two modes - data collection mode and results processing mode.

The back-end has been developed with couchdb. The primary concepts of the database are:

  users
  image-compare-lists
  tasks
  results

Some users, let's call them "configurators" are able to define a set of images pairs that should be compared and the order in which the user should compare them. These ordered sets of pairs of images are called image-compare-lists. The configurator is then able to assign a particular list to a user. This assignment is called a task. Each time a user picks between two images, the information about which user, which images where compared, the task that was being worked on, and the time of the comparison is recorded in a result.

The front end, in data collection mode, will allow a user to log in, see the set of tasks that are assigned to them, and select a task to work on. At that point they should be shown two images and a criterion by which to compare them. For example, they might see two retinal scans and be asked which shows more severe indications of retinopathy. After each choice, a result is recorded in the database, and the user is shown how many more comparisons they have left in this task.

In results comparison mode, some users will be able to see statistics and diagrams showing amortized information. For example, they should be able to see which tasks have been assigned to which users, how many users have completed their tasks. But more importantly, they should be able to see patterns in the results. Do the experts agree on which images show the greatest signs of retinopathy?  Which images are "controverial"?  Are the experts "internally consistent"? That is, if image A is worse than B, and B is worse than C is A also worse than C. Which images is that not true for?  Do other experts have inconsistencies for the same images?


to run locally, python -m SimpleHTTPServer
